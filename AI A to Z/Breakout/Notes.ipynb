{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Why is it hard? The ball is small and moving \n",
    "In the code we are implementing the Version from Adam Paszke, Creator of pytorch\n",
    "Useful is the model.py & train.py\n",
    "The envs is to normalize the env and provided by openai\n",
    "Main will just run them\n",
    "Test.py is just to test it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9a0710010073e88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now working on model.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efc85d1dde04f3b3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-18T06:22:47.240936300Z",
     "start_time": "2023-10-18T06:22:47.216902500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 32 x 3 x 3 -> 32 output-filters x (heights x width) \n",
    " \n",
    "# Initializing and setting the variance of a tensor of weights\n",
    "def normalized_columns_initializer(weights, std=1.0):  # We will use small std for the actor and bigger for critic\n",
    "    out = torch.randn(weights.size())\n",
    "    out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out))  # Var(out) = std^2\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__ # python trick that will look for the type of connection in the object \"m\" (convolution or full connection)\n",
    "    if classname.find('Conv') != -1: # if the connection is a convolution\n",
    "        weight_shape = list(m.weight.data.size()) # list containing the shape of the weights in the object \"m\"\n",
    "        fan_in = np.prod(weight_shape[1:4]) # dim1 * dim2 * dim3\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0] # dim0 * dim2 * dim3\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound\n",
    "        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights\n",
    "        m.bias.data.fill_(0) # initializing all the bias with zeros\n",
    "    elif classname.find('Linear') != -1: # if the connection is a full connection\n",
    "        weight_shape = list(m.weight.data.size()) # list containing the shape of the weights in the object \"m\"\n",
    "        fan_in = weight_shape[1] # dim1\n",
    "        fan_out = weight_shape[0] # dim0\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound\n",
    "        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights\n",
    "        m.bias.data.fill_(0) # initializing all the bias with zeros\n",
    "        \n",
    "        \n",
    "class ActorCritic(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1) # first convolution\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1) # second convolution\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1) # third convolution\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1) # fourth convolution\n",
    "        self.lstm = nn.LSTMCell(32 * 3 * 3, 256) # making an LSTM (Long Short Term Memory) to learn the temporal properties of the input - we obtain a big encoded vector S of size 256 that encodes an event of the game\n",
    "        # num_outputs = action_space.n # getting the number of possible actions\n",
    "        num_outputs = 4\n",
    "        self.critic_linear = nn.Linear(256, 1) # full connection of the critic: output = V(S)\n",
    "        self.actor_linear = nn.Linear(256, num_outputs) # full connection of the actor: output = Q(S,A)\n",
    "        self.apply(weights_init) # initializing the weights of the model with random weights\n",
    "        # self.actor_linear.weight.data = normalized_columns_initializer(self.actor_linear.weight.data, 0.01) # setting the standard deviation of the actor tensor of weights to 0.01\n",
    "        self.actor_linear.bias.data.fill_(0) # initializing the actor bias with zeros\n",
    "        # self.critic_linear.weight.data = normalized_columns_initializer(self.critic_linear.weight.data, 1.0) # setting the standard deviation of the critic tensor of weights to 0.01\n",
    "        self.critic_linear.bias.data.fill_(0) # initializing the critic bias with zeros\n",
    "        self.lstm.bias_ih.data.fill_(0) # initializing the lstm bias with zeros\n",
    "        self.lstm.bias_hh.data.fill_(0) # initializing the lstm bias with zeros\n",
    "        self.train() # setting the module in \"train\" mode to activate the dropouts and batch-norms\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs # getting separately the input images to the tuple (hidden states, cell states)\n",
    "        x = F.elu(self.conv1(inputs)) # forward propagating the signal from the input images to the 1st convolutional layer\n",
    "        x = F.elu(self.conv2(x)) # forward propagating the signal from the 1st convolutional layer to the 2nd convolutional layer\n",
    "        x = F.elu(self.conv3(x)) # forward propagating the signal from the 2nd convolutional layer to the 3rd convolutional layer\n",
    "        x = F.elu(self.conv4(x)) # forward propagating the signal from the 3rd convolutional layer to the 4th convolutional layer\n",
    "        x = x.view(-1, 32 * 3 * 3) # flattening the last convolutional layer into this 1D vector x\n",
    "        hx, cx = self.lstm(x, (hx, cx)) # the LSTM takes as input x and the old hidden & cell states and ouputs the new hidden & cell states\n",
    "        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)\n",
    "        return self.critic_linear(x), self.actor_linear(x), (hx, cx) # returning the output of the critic (V(S)), the output of the actor (Q(S,A)), and the new hidden & cell states ((hx, cx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![The Basic Adam algo that we will implement](img/adam_algo.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d1131de1265807b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Implementing the Adam optimizer with shared states\n",
    "\n",
    "class SharedAdam(optim.Adam): # object that inherits from optim.Adam\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay) # inheriting from the tools of optim.Adam\n",
    "        for group in self.param_groups: # self.param_groups contains all the attributes of the optimizer, including the parameters to optimize (the weights of the network) contained in self.param_groups['params']\n",
    "            for p in group['params']: # for each tensor p of weights to optimize\n",
    "                state = self.state[p] # at the beginning, self.state is an empty dictionary so state = {} and self.state = {p:{}} = {p: state}\n",
    "                state['step'] = torch.zeros(1) # counting the steps: state = {'step' : tensor([0])}\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_() # the update of the adam optimizer is based on an exponential moving average of the gradient (moment 1)\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_() # the update of the adam optimizer is also based on an exponential moving average of the squared of the gradient (moment 2)\n",
    "\n",
    "    # Sharing the memory\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_() # tensor.share_memory_() acts a little bit like tensor.cuda()\n",
    "                state['exp_avg'].share_memory_() # tensor.share_memory_() acts a little bit like tensor.cuda()\n",
    "                state['exp_avg_sq'].share_memory_() # tensor.share_memory_() acts a little bit like tensor.cuda()\n",
    "\n",
    "    # Performing a single optimization step of the Adam algorithm (see algorithm 1 in https://arxiv.org/pdf/1412.6980.pdf)\n",
    "    def step(self):     # this is the same as using super(SharedAdam, self).step()\n",
    "        loss = None\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                bias_correction1 = 1 - beta1 ** state['step'][0]\n",
    "                bias_correction2 = 1 - beta2 ** state['step'][0]\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a6f43805d35a795"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from Code_With_Comments.envs import create_atari_env\n",
    "from Code_With_Comments.model import ActorCritic\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Implementing a function to make sure the models share the same gradient\n",
    "def ensure_shared_grads(model, shared_model):\n",
    "    for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "        if shared_param.grad is not None:\n",
    "            return\n",
    "        shared_param._grad = param.grad\n",
    "        \n",
    "def train(rank, params, shared_model, optimizer):\n",
    "    # We have to desynchronize the agents (we do this with rank (rank = #agents))\n",
    "    torch.manual_seed(params.seed + rank) # Sets different random seed generator for each agent (for the randn function)\n",
    "    env = create_atari_env(params.env_name)\n",
    "    env.seed(params.seed + rank)\n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "    state = env.reset() \n",
    "    state = torch.from_numpy(state)\n",
    "    done = True\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        episode_length += 1\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "        if done:\n",
    "            cx = Variable(torch.zeros(1,256))\n",
    "            hx = Variable(torch.zeros(1,256))\n",
    "        else:\n",
    "            cx = Variable(cx.data)\n",
    "            hx = Variable(hx.data)\n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "        for step in range(params.num_steps):\n",
    "            value, action_values, (hx, cx) = model(Variable(state.unsqueeze(0)), (hx, cx))  # inputs\n",
    "            prob = F.softmax(action_values)\n",
    "            log_prob = F.log_softmax(action_values)\n",
    "            entropy = (log_prob * prob).sum(1)\n",
    "            entropies.append(entropy)\n",
    "            action = prob.multinomial().data    # pick an action randomly\n",
    "            log_prob = log_prob.gather(1, Variable(action))     # he get the log prob associated with the action chosen\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            state, reward, done = env.step(action.numpy())\n",
    "            done = (done or episode_length >= params.max_episode_legth)\n",
    "            reward = max(min(reward, 1), -1) # ensure that -1 <= reward <= 1\n",
    "            if done:\n",
    "                episode_length = 0\n",
    "                state = env.reset()\n",
    "            state = torch.from_numpy(state)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "            # We have done the exploration now we have to update the network\n",
    "            R = torch.zeros(1, 1)\n",
    "            if not done:\n",
    "                value, _, _  = model(Variable(state.unsqueeze(0)), (hx, cx))    # Get just the new value\n",
    "                R = value.data\n",
    "            values.append(Variable(R))\n",
    "            policy_loss = 0\n",
    "            value_loss = 0\n",
    "            R = Variable(R)\n",
    "            gae = torch.zeros(1, 1)     # Generalized advantage estimation A(a,s) = Q(a, s) - V(s)\n",
    "            for i in reversed(range(len(rewards))):     # We are moving back in time\n",
    "                R = params.gamma * R  + rewards[i]  # R = r_0 + gamma * r_1 + gamma ^ 2 * r_2 + ... + gamma^(n-1) * r_{n-1} + gamma^nb_steps * V(last_state)\n",
    "                advantage = R - values[i]\n",
    "                value_loss = value_loss + 0.5 * advantage.pow(2)    # Q*(a*, s) = V*(s)\n",
    "                # Now to get the policy loss we need the gae witch we get from Temporal-Difference\n",
    "                TD = rewards[i] + params.gamma * values[i+1].data - values[i].data \n",
    "                gae = gae * params.gamma * params.tau + TD  # gae = sum_i (gamma*tau)^i * TD(i)\n",
    "                policy_loss = policy_loss - log_probs[i] * Variable(gae) - 0.01 * entropies[i]  # policy_loss = -sum_i log(pi_i) * gae + 0.01 * H_i , where H = entropy\n",
    "            # Now just do SGD\n",
    "            optimizer.zero_grad()\n",
    "            (policy_loss + 0.5 * value_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 40)    \n",
    "            ensure_shared_grads(model, shared_model)\n",
    "            optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T07:16:47.632136900Z",
     "start_time": "2023-10-18T07:16:47.300253700Z"
    }
   },
   "id": "70337c446b4c5872"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
